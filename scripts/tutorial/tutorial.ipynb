{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "First, we need to build datasets for training, validating and testing a neural ranking model.\n",
    "\n",
    "For training, we use the qrel of a competition (e.g., TREC robust04, NTCIR WWW), which contains the relevance judgements for query - doc pairs. Since qrel only contains query ids and doc ids, we need to extract queries and docuemnts from topic file and the corpus respectively.\n",
    "\n",
    "For test/validation, we usually use neural ranking to rerank the first k documents retreived by a simple retrieval model (e.g., BM25) since neural ranking is too slow to compute the relevance score for every document in the corpus (eg., Clueweb12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Fulvus server,  Robust04 corpus is indexed by Anserini with `-transformed` option (For more details, check https://github.com/castorini/anserini).\n",
    " \n",
    "The path of robust04 index is \n",
    "```/ir/index/lucene-index.robust04.pos+docvectors+rawdocs+transformed ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,  build the robust04 datasets using the `build_datapack.py` script:\n",
    "\n",
    "```shell\n",
    "cd neural_ranking\n",
    "\n",
    "python scripts/utils/build_datapack.py \n",
    "--index /ir/index/lucene-index.robust04.pos+docvectors+rawdocs+transformed \n",
    "--topic ./resources/topics_and_qrels/topics.robust04.301-450.601-700.txt \n",
    "--qrel ./resources/topics_and_qrels/qrels.robust2004.txt \n",
    "--output built_data/robust04\n",
    "```\n",
    "\n",
    "Then you will get the robust04 dataset for training and valdidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\"))\n",
    "\n",
    "import neural_ranking\n",
    "import matchzoo as mz\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from neural_ranking.runners.dataset import ReRankDataset\n",
    "from neural_ranking.runners.utils import  ReRankTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss and Metrics\n",
    "\n",
    "ranking_task = mz.tasks.Ranking(mz.losses.RankHingeLoss())\n",
    "ranking_task.metrics = [\n",
    "    mz.metrics.NormalizedDiscountedCumulativeGain(k=20),\n",
    "    mz.metrics.Precision(k=30),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReRankDataset(\"robust04\", rerank_hits=1000, debug_mode=True) # debug mode will only load 100 docs from the dataset\n",
    "dataset.init_topic_splits(dev_ratio=0.2, test_ratio=0, seed=2020) # split data into train and dev randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-fill missing configs with default settings\n",
    "model,preprocessor,dataset_builder, dataloader_builder = mz.auto.prepare(\n",
    "            task=ranking_task,\n",
    "            model_class=mz.models.Bert,\n",
    "            data_pack=dataset.pack,\n",
    "            embedding=None, # Bert does not need embedding\n",
    "            preprocessor=mz.models.Bert.get_default_preprocessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multi-Core Processing text_left with bert_encode: 100%|██████████| 250/250 [00:00<00:00, 3130.26it/s]\n",
      "Multi-Core Processing text_right with bert_encode: 100%|██████████| 1000/1000 [00:02<00:00, 462.53it/s]\n",
      "Processing text_left with Chain Transform of TruncatedLength: 100%|██████████| 230/230 [00:00<00:00, 258699.36it/s]\n",
      "Processing text_right with Chain Transform of TruncatedLength: 100%|██████████| 992/992 [00:00<00:00, 501174.36it/s]\n",
      "Multi-Core Processing length_left with len: 100%|██████████| 250/250 [00:00<00:00, 44987.82it/s]\n",
      "Multi-Core Processing length_right with len: 100%|██████████| 1000/1000 [00:00<00:00, 24957.48it/s]\n",
      "Multi-Core Processing text_left with bert_encode: 100%|██████████| 250/250 [00:00<00:00, 3089.59it/s]\n",
      "Multi-Core Processing text_right with bert_encode: 100%|██████████| 500/500 [00:01<00:00, 431.50it/s]\n",
      "Processing text_left with Chain Transform of TruncatedLength: 100%|██████████| 213/213 [00:00<00:00, 158767.86it/s]\n",
      "Processing text_right with Chain Transform of TruncatedLength: 100%|██████████| 499/499 [00:00<00:00, 392086.49it/s]\n",
      "Multi-Core Processing length_left with len: 100%|██████████| 250/250 [00:00<00:00, 47608.44it/s]\n",
      "Multi-Core Processing length_right with len: 100%|██████████| 500/500 [00:00<00:00, 27527.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess query and document using BertPreprocessor\n",
    "dataset.apply_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ the pair-wise loss function (i.e., Hinge Loss), which requires to sample a positive example and a negative example from the training data, so we need a data loader to handle the sampling and batching work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset and dataloader. For example, we need to sample positive and negative examples for training, but not for evaluation (test/dev).\n",
    "\n",
    "def get_dataloaders(dataset, dataset_builder, dataloader_builder, batch_size=2):\n",
    "    training_pack = dataset.train_pack_processed\n",
    "    # Setup data\n",
    "    trainset = dataset_builder.build(\n",
    "        training_pack,\n",
    "        batch_size=batch_size,\n",
    "        sort=False,\n",
    "    )\n",
    "    train_loader = dataloader_builder.build(trainset)\n",
    "\n",
    "    eval_dataset_kwargs = copy.copy(dataset_builder._kwargs)\n",
    "    eval_dataset_kwargs[\"batch_size\"] = batch_size * 2\n",
    "    eval_dataset_kwargs[\"shuffle\"] = False\n",
    "    eval_dataset_kwargs[\"sort\"] = False\n",
    "    eval_dataset_kwargs[\"resample\"] = False\n",
    "    eval_dataset_kwargs[\"mode\"] = \"point\"\n",
    "\n",
    "    eval_dataset_builder = mz.dataloader.DatasetBuilder(\n",
    "        **eval_dataset_kwargs,\n",
    "    )\n",
    "    \n",
    "    dev_dataset = eval_dataset_builder.build(dataset.dev_pack_processed)\n",
    "    dev_loader = dataloader_builder.build(dataset=dev_dataset, stage=\"dev\")\n",
    "    return train_loader, dev_loader\n",
    "\n",
    "train_loader, dev_loader = get_dataloaders(dataset, dataset_builder, dataloader_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "trainer = ReRankTrainer(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            trainloader=train_loader,\n",
    "            validloader=dev_loader,\n",
    "            epochs=3,\n",
    "            patience=2,\n",
    "            device=\"cuda\",\n",
    "            save_dir=\"checkpoint\",\n",
    "            fp16=False,\n",
    "            clip_norm=5,\n",
    "            batch_accumulation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc4082c5b5441cf990b15922906a669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter-25 Loss-0.974]:\n",
      "  Validation: normalized_discounted_cumulative_gain@20(0.0): 0.0306 - precision@30(0.0): 0.0018\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fffe544e5da41a6a41733a69660eaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter-50 Loss-1.026]:\n",
      "  Validation: normalized_discounted_cumulative_gain@20(0.0): 0.0441 - precision@30(0.0): 0.0018\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cce39581b9e419bba1f1d1073daceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter-75 Loss-1.090]:\n",
      "  Validation: normalized_discounted_cumulative_gain@20(0.0): 0.0306 - precision@30(0.0): 0.0018\n",
      "\n",
      "Cost time: 19.354414463043213s\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}